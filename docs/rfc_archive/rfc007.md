RFC-V4-007 — IVM Sink Orchestration, Triggering, and Plugin-based Delivery

IVM Sink Orchestration, Triggering, and Plugin-based Delivery (Sync-first + Outbox)

Status: Final → **ADR로 마이그레이션됨** ([ADR-0007](../adr/0007-sink-orchestration.md))
Created: 2026-01-25
Scope: v4.1 Sink Layer (ivm-lite/rawdata, slicing, view, sink, shared/contract-registry, apps/runtimeapi, apps/sinkworker)
Depends on: RFC-V4-001, RFC-V4-002, RFC-V4-003, RFC-V4-004, RFC-V4-005, RFC-V4-006
Audience: Platform / Architecture / Sink Developers
Non-Goals: 검색/추천 모델 자체 구현, 외부 시스템 운영 전략, 실시간 사용자 서빙 API

> **참고**: 이 RFC는 ADR-0007로 마이그레이션되었습니다. 핵심 결정사항은 ADR을 참고하세요.
> 상세한 설계 내용은 이 RFC 문서를 참고하세요.

0. Executive Summary

본 RFC는 Slice/View 결과를 정책 기반(SinkRule)으로 외부 시스템(OpenSearch, Reco 등)에 전달하는 Sink Orchestration 아키텍처를 정의한다.

**핵심 결론**:
- Sink는 pull 스캐너가 아니다
- Sink는 항상 트리거 기반(Task 기반)으로 동작하며, Slice/View를 Port를 통해 읽어서 외부로 전달한다
- 실행 로직의 SSOT는 SinkRule(정책) + PluginRegistry(실행 연결) + SinkPlan(실행 순서)이다
- v0는 sync-first로 API 요청이 곧 트리거이며, v1+는 outbox/eventbus를 트리거로 하되 동일 SinkOrchestrator + Plugin이 실행된다
- **Sync / Outbox / EventBus subscriber는 트리거 방식의 차이일 뿐이며, 실행은 동일 SinkOrchestrator + SinkPlugin으로 수행한다**
- OpenSearch upsert, Reco(AWS Personalize) 전달은 SinkPlugin의 책임이다
- 설계상 확장성은 열려있음 (나중에 비동기 subscriber가 처리하는 형태로 자연스럽게 확장 가능)

**RFC-001~006과의 일관성**:
- RFC-001: Slice/View 개념, Inverted Index (검색엔진 인덱스와 구분) (RFC-001: 17-1)
- RFC-002: Canonicalization/Hashing 규칙 준수
- RFC-003: Contract Registry 기반 SinkRule 관리
- RFC-004: Replay/Backfill 시 Sink 재적재 순서 (RFC-004: 13)
- RFC-005: Domain-sliced 구조 (sink 도메인)
- RFC-006: RefIndexSlice/EnrichmentSlice와의 연계

1. 전체 파이프라인 개요 (Chronological)

```
[Trigger]
  ├─ Sync: runtimeapi 호출
  ├─ Async (Outbox): outbox 이벤트
  └─ Async (EventBus): Event Bus 이벤트 (확장 가능)

→ TaskSpec / TaskId 확정
→ RawData persist
→ Slice build (Core / RefIndex / Enriched)
→ View materialize (필요 시)
→ SinkPlan 계산 (SinkRule 기반)
→ SinkOrchestrator 실행 (트리거 방식과 무관하게 동일)
→ SinkPlugin이 Slice/View를 읽어서 외부 시스템에 전달
→ CommitRef 생성
```

**확장성**: 트리거는 교체 가능하며, 실행 파이프라인은 동일함.

**RFC-001과의 연계**:
- Slice 생성은 RFC-001의 slicing() 모드 (FULL/INCREMENTAL) 준수
- View 생성은 RFC-001의 Virtual View 개념 준수
- ChangeSet 기반 증분 업데이트 시 Sink 재적재 (RFC-001: 9-3)

2. 핵심 불변식 (Non-Negotiable)

**2-1. Sink 실행 원칙 (P0, 필수)**

- Sink는 Slice를 알아서 읽지 않는다
- Sink 실행은 반드시 트리거(Task)가 있어야 한다
- Slice/View 데이터는 outbox/이벤트에 실리지 않는다
- SinkPlugin은 Port를 통해 Slice/View를 읽는다
- OpenSearch upsert는 SinkPlugin이 수행한다
- **Sync / Outbox / EventBus subscriber는 트리거 방식의 차이일 뿐이며, 실행은 동일 SinkOrchestrator + SinkPlugin으로 수행한다**

**2-2. 확장성 원칙 (P0, 필수)**

설계상 확장성은 열려있음. 트리거를 교체 가능하게 잡았기 때문에, 나중에 "비동기 subscriber(consumer)가 처리"하는 형태로 자연스럽게 확장 가능함.

**핵심**: 실행 로직(SinkOrchestrator + SinkPlugin)은 그대로 유지하고, 트리거만 outbox/이벤트 버스로 바꾸는 것임.

**2-3. 결정성 및 멱등성 (RFC-002 준수)**

- 동일 TaskId 재실행 시 동일 결과 보장 (멱등성)
- SinkPlugin 실행은 결정적이어야 함 (시간/랜덤 금지)
- CommitRef는 결정적으로 생성됨 (RFC-002: 1-3)
- 멱등 키는 taskId로 고정 (subscriber가 중복 수신해도 taskId로 dedupe 가능)

3. 트리거 모델 (가장 중요한 섹션)

**3-1. Sync-first (v0)**

**트리거 주체**: apps/runtimeapi

**트리거 방식**: API 호출

**실행 주체**: runtimeapi 프로세스

**실행 흐름**:
1. API 요청 수신
2. Slice/View 생성 (RFC-001: slicing(), queryView())
3. SinkOrchestrator.run(SinkPlan)
4. SinkPlugin이 Slice/View를 읽어 OpenSearch/Reco로 전달

**즉, API 요청 = Sink 트리거**

**3-2. Outbox 기반 Async (v1+)**

**트리거 주체**: outbox 이벤트

**실행 주체**: apps/sinkworker

**역할 분리**:
- runtimeapi: TaskSpec + SinkPlanRef를 outbox에 적재
- sinkworker: outbox consume 후 SinkOrchestrator 실행

**즉, outbox 이벤트 = Sink 트리거**

**3-3. Event Bus 기반 Subscriber (v1+, 확장 가능)**

**트리거 주체**: Event Bus (예: Kafka, SNS/SQS)

**실행 주체**: 독립 subscriber 프로세스 (예: opensearch-subscriber, personalize-subscriber)

**역할 분리**:
- runtimeapi: Slice build 완료 시 `SliceBuilt` 또는 `TaskReadyForSink` 이벤트 발행
- subscriber: 해당 이벤트를 consume → SinkOrchestrator 실행

**장점**:
- 여러 subscriber(예: opensearch, personalize, data-lake)를 병렬로 붙이기 쉬움
- 팬아웃/확장성 담당

**SOTA 패턴**:
- Outbox + EventBus를 같이 둠
- Outbox는 "정합/내구/멱등" 보장
- EventBus는 "팬아웃/확장성" 담당

**즉, Event Bus 이벤트 = Sink 트리거 (확장 가능)**

**3-4. 트리거 방식 통합 원칙 (P0, 필수)**

**Sync / Outbox / EventBus subscriber는 트리거 방식의 차이일 뿐이며, 실행은 동일 SinkOrchestrator + SinkPlugin으로 수행한다**

**불변 조건**:
1. Sink 실행 입력은 항상 Ref로만 전달
2. SinkOrchestrator는 "순수 함수형 실행기"처럼 유지
3. 멱등 키는 taskId로 고정

**3-5. Outbox/EventBus 이벤트 구조 (P0, 필수)**

```kotlin
// Outbox 또는 EventBus 이벤트 공통 구조
data class SinkTriggerEvent(
    val taskId: String,  // UUID (멱등성 키로 고정)
    val sinkPlanRef: ContractRef?,  // SinkPlan 계약 참조 (optional)
    val sinkRuleIds: List<ContractRef>?,  // 또는 SinkRule 목록 (optional)
    val inputRefs: InputRefs,  // sliceRefs[] 또는 viewRef
    val policyRefs: PolicyRefs?,  // mappingId/schemaVersion (optional)
    val attempt: Int = 1,
    val dedupeKey: String = taskId  // 멱등성 키 (taskId로 고정)
)

data class InputRefs(
    val sliceRefs: List<SliceRef>?,
    val viewRef: ViewRef?
)

data class PolicyRefs(
    val mappingId: String?,
    val schemaVersion: String?
)
```

**Outbox/EventBus에 넣지 않는 것 (P0, 필수)**:
- Slice 데이터 ❌
- View 결과 ❌
- 대용량 payload ❌
- 실데이터 ❌

**이유**:
- 재실행/리플레이 불가
- 중복/폭발
- 결정성 붕괴
- 확장성 제한

**RFC-008과의 일관성**:
- Outbox는 비동기 경계에서만 사용 (RFC-008: 6-1, 6-2)
- compile.async() 또는 ship.async() 시에만 outbox 사용 (RFC-008: 6-1)
- Outbox Task 모델은 RFC-008: 7에서 정의됨 (RFC-008: 7-1, 7-2, 7-3)

**3-6. "열려있게" 만드는 불변 조건 (P0, 필수)**

**3-6-1. Sink 실행 입력은 항상 Ref로만 전달**

outbox든 이벤트든 payload는 반드시:
- taskId
- sinkPlanRef 또는 sinkRuleIds
- inputRefs (sliceRefs/viewRef)
- policyRefs (mappingId/schemaVersion)

이 형태로 고정. 실데이터 넣기 금지.

**3-6-2. SinkOrchestrator는 "순수 함수형 실행기"처럼 유지**

트리거 계층(runtimeapi/worker/subscriber)에서 비즈니스 로직 금지.

subscriber는 "받아서 run"만 함.

**3-6-3. 멱등 키는 taskId로 고정**

subscriber가 중복 수신해도 taskId로 dedupe 가능.

CommitRef에도 taskId가 들어가야 함.

4. "Slice에 있는 걸 누가, 어떻게 읽는가"

**4-1. 읽는 주체**

SinkPlugin (예: opensearch-indexer@1, personalize-feed-publisher@1)

**4-2. 읽는 방법**

**직접 DB 접근 ❌**

**반드시 Port 계약 사용 ⭕**

**필수 Port (RFC-005 준수)**:
- `SliceReaderPort` 또는 `SliceBatchReaderPort` (RFC-005: slicing/ports, view/ports)
- View의 경우 `SliceBatchReaderPort` 사용 (RFC-005: view/ports)

**최소 API**:
```kotlin
interface SliceReaderPort {
    suspend fun getManifest(sliceRef: SliceRef): Either<ReadError, SliceManifest>
    suspend fun readBatch(sliceRef: SliceRef, cursor: String?, limit: Int): Either<ReadError, SliceBatch>
}

interface SliceBatchReaderPort {
    suspend fun readBatch(sliceRefs: List<SliceRef>, cursor: String?, limit: Int): Either<ReadError, SliceBatch>
}

interface ViewReaderPort {
    suspend fun getManifest(viewRef: ViewRef): Either<ReadError, ViewManifest>
    suspend fun readBatch(viewRef: ViewRef, cursor: String?, limit: Int): Either<ReadError, ViewBatch>
}
```

**참고**: View는 여러 Slice를 조합하므로 `SliceBatchReaderPort`를 사용 (RFC-005: 6-3)

**4-3. 읽기 방식**

- 배치/스트리밍 방식
- cursor 기반 페이지네이션
- 대용량 slice도 안정적으로 처리 가능

**4-4. RFC-001, RFC-005와의 일관성**

- SliceReaderPort/SliceBatchReaderPort는 RFC-001의 Slice 저장소를 읽기 전용으로 접근
- ViewReaderPort는 RFC-001의 Virtual View 개념을 구현
- Port를 통한 접근으로 도메인 경계 유지 (RFC-005: 7-1)
- View는 SliceBatchReaderPort를 통해 여러 Slice를 배치로 읽음 (RFC-005: 6-3)

5. SinkRule / SinkPlan / Plugin 구조

**5-1. SinkRule (정책, Contract Registry 저장)**

어떤 입력을, 어떤 타겟으로, 어떤 매핑/전달/커밋 규칙으로 보낼지 정의

```yaml
id: sinkrule.opensearch.product-search
version: 1.0.0
status: ACTIVE

input:
  type: SLICE
  sliceTypes: [PRODUCT_CORE, PRODUCT_DISCOVERY]
  filter: # optional
    tenantId: "tenant1"

target:
  type: OPENSEARCH
  index: "product-search-global"
  alias: "product-search-active"

mapping:
  # mapping DSL
  docIdSpec:  # RFC-003: 2-10 준수
    scope: SLICE  # 또는 VIEW
    pattern: "{entityKey}#{version}"  # 멱등성 키 (RFC-004: 14)
  fields:
    - from: "CORE.title"
      to: "title"
    - from: "DISCOVERY.tokens"
      to: "tokens"
  
**DocIdSpec 규칙 (RFC-003: 2-10)**:
- doc_id는 Sink의 SSOT (멱등성 보장)
- doc_id 패턴 변경은 MAJOR 버전 변경 필요
- SinkRule 계약에 docIdSpec 필수

commit:
  strategy: ALIAS_SWAP
  batchSize: 1000
```

**5-2. SinkPlan**

단일 Task에서 실행할 SinkRule들의 총순서

```yaml
id: sinkplan.product-full-sync
version: 1.0.0

rules:
  - sinkRuleRef: sinkrule.opensearch.product-search@1.0.0
    order: 1
  - sinkRuleRef: sinkrule.personalize.product-feed@1.0.0
    order: 2
```

**5-3. SinkPlugin**

SinkRule을 실제로 수행하는 실행체

입력/출력 계약 고정

**Plugin Registry 구조**:
- Plugin은 Contract Registry에 등록됨 (RFC-003: 2-16)
- Plugin 실행은 결정적이어야 함 (RFC-002: 1-3)

6. OpenSearch SinkPlugin (예시)

**6-1. 책임**

- Slice/View를 읽는다 (SliceReaderPort/SliceBatchReaderPort/ViewReaderPort 사용)
- Mapping DSL 적용
- Bulk upsert 수행
- Alias swap 등 커밋 정책 수행

**6-2. 내부 파이프라인**

1. ResolveInput (SliceReaderPort 또는 SliceBatchReaderPort)
2. Transform (mapping DSL, RFC-002: Canonicalization 규칙 준수)
3. Bulk Upsert (OpenSearch, doc_id 기반 멱등성 보장)
4. Commit (alias swap or upsert)
5. CommitRef 생성 (RFC-002: Hashing 규칙 준수)

**6-3. CommitRef**

```json
{
  "type": "opensearch",
  "index": "search-global-3",
  "alias": "search-active-global",
  "docCount": 120341,
  "taskId": "task_...",
  "commitHash": "sha256:...",  // 결정성 보장 (RFC-002: 2-1)
  "docIds": ["entityKey1#v1", "entityKey2#v2"]  // doc_id 기반 멱등성 (RFC-004: 14)
}
```

**멱등성 보장 (RFC-004: 14)**:
- Sink write는 doc_id 기반 idempotent
- 동일 doc_id 재실행 시 동일 결과 반환
- 중복 CDC 이벤트 허용

**6-4. RFC-001과의 구분**

- 본 RFC의 OpenSearch는 검색엔진 인덱스 (RFC-001: 17-1)
- RFC-001의 inverted index는 런타임 보조 인덱스 (엔티티 간 조인용)
- 두 개념은 완전히 분리됨

7. Reco / AWS Personalize SinkPlugin (예시)

**7-1. 책임**

- RecoFeedSlice/View 읽기 (SliceReaderPort/SliceBatchReaderPort 사용)
- CSV/JSONL 직렬화 (RFC-002: Canonicalization 규칙 준수)
- S3 drop
- Dataset Import Job 트리거

**7-2. 내부 파이프라인**

1. ResolveInput (SliceReaderPort 또는 SliceBatchReaderPort)
2. Validate schema
3. Serialize (CSV/JSONL, 결정적 직렬화)
4. Deliver (S3)
5. Import Job 실행
6. CommitRef 생성 (RFC-002: Hashing 규칙 준수)

8. 누가 무엇을 "한다" 요약표

| 단계 | 주체 |
|------|------|
| Slice 생성 | slicing domain (RFC-001) |
| View 생성 | view domain (RFC-001) |
| Sink 실행 트리거(sync) | runtimeapi |
| Sink 실행 트리거(async - outbox) | outbox |
| Sink 실행 트리거(async - eventbus) | Event Bus (Kafka, SNS/SQS 등) |
| Sink 실행 주체 | runtimeapi / sinkworker / subscriber |
| Slice/View 읽기 | SinkPlugin (Port 사용) |
| OpenSearch upsert | opensearch-indexer plugin |
| Reco 전달 | personalize-feed-publisher plugin |

**확장성**: 트리거 방식(sync/outbox/eventbus)은 교체 가능하며, 실행 로직(SinkOrchestrator + SinkPlugin)은 동일함.

9. 절대 금지 패턴 (P0, 필수)

**9-1. 금지 사항**

- Sink가 주기적으로 slice를 스캔 ❌
- Slice 저장소를 직접 쿼리 ❌
- outbox/eventbus에 실데이터 적재 ❌
- Sink 로직을 API 레이어에 구현 ❌
- 트리거 계층에서 비즈니스 로직 구현 ❌

**9-2. 허용 패턴**

- 트리거 기반 실행만 허용 ⭕
- Port를 통한 Slice/View 읽기만 허용 ⭕
- Outbox/EventBus에는 참조만 저장 ⭕
- SinkPlugin은 독립 도메인으로 분리 ⭕
- 트리거 방식(sync/outbox/eventbus) 교체 가능 ⭕
- Subscriber는 "받아서 run"만 수행 ⭕

10. Replay/Backfill과의 연계 (RFC-004 준수)

**10-1. Replay 시 Sink 재적재**

Replay 실행 시 (RFC-004: 13):
1. Slice 계산 (FULL 또는 INCREMENTAL slicing)
2. Inverted index 재계산 (RFC-001: 17-6-1)
3. **Sink 재적재** (SinkOrchestrator 실행)
   - SinkPlan 계산 (SinkRule 기반)
   - SinkPlugin이 Slice/View를 읽어서 외부 시스템에 전달
   - CommitRef 생성
4. VerifyMode 기준 검증

**중요**: Sink 재적재는 Slice/Inverted Index 재계산 후에만 수행 (순서 위반 시 실행 차단)

**10-2. Sink 재적재 규칙 (RFC-004 준수)**

- Replay는 동일 SinkPlan으로 재실행
- SinkPlugin은 멱등적으로 동작해야 함 (doc_id 기반 idempotent, RFC-004: 14)
- CommitRef는 결정적으로 생성됨
- 중복 CDC 이벤트 허용 (RFC-004: 14)

11. Domain-sliced 구조 (RFC-005 준수)

**11-0. 최종 형태**

v0는 단일 모듈로 시작하되, 디렉터리 구조를 그대로 멀티모듈로 승격 가능하게 고정하는 형태가 최종임.

**핵심 원칙**:
- 도메인 단위 슬라이싱 + 도메인 내부 헥사고날 유지
- 트리거 계층은 apps/*로 분리
- sink 플러그인은 sink/adapters/plugins/*로 관리하는 구조가 SOTA임

**11-1. v0 단일 모듈 레포 구조 (최종)**

```
ivm-lite/
  README.md
  build.gradle.kts
  settings.gradle.kts

  docs/
    rfc/
      RFC-V4-007-sink-orchestration.md

  src/main/kotlin/com/company/ivmlite/
    shared/
      domain/
        ids/
          TenantId.kt
          EntityKey.kt
          Version.kt
          Hash.kt
          Refs.kt
        errors/
          DomainError.kt
          ErrorCodes.kt
        determinism/
          CanonicalJson.kt
          CanonicalSort.kt
          Sha256.kt
      ports/
        ClockPort.kt
        SingleFlightPort.kt
        ContractRegistryPort.kt
        ObservabilityPort.kt
        TenantValidatorPort.kt
      adapters/
        clock/
          SystemClockAdapter.kt
        canonicaljson/
          Rfc8785CanonicalJson.kt
        hashing/
          Sha256Hasher.kt
      wiring/
        AppWiring.kt
        RegistryWiring.kt
        StorageWiring.kt

    rawdata/
      domain/
        RawRecord.kt
        RawRef.kt
      application/
        PutRawUseCase.kt
      ports/
        RawRepositoryPort.kt
      adapters/
        dynamodb/
          RawRepositoryAdapter.kt

    changeset/
      domain/
        ChangeSetV1.kt
        DiffKind.kt
        FieldPath.kt
        FanoutSource.kt
      application/
        ComputeChangeSetUseCase.kt
        ImpactResolver.kt
      ports/
        ChangeSetRepositoryPort.kt
        ContractRegistryPort.kt
      adapters/
        dynamodb/
          ChangeSetRepositoryAdapter.kt

    slicing/
      domain/
        Slice.kt
        SliceRef.kt
        SliceType.kt
        JoinSpec.kt
        RefIndexSlice.kt
        EnrichmentSlice.kt
        InvertedIndexKey.kt
        InvertedIndexEntry.kt
        SlicingEngine.kt
      application/
        BuildSlicesUseCase.kt
        validators/
          SliceInvariantValidator.kt
      ports/
        SliceRepositoryPort.kt
        InvertedIndexRepositoryPort.kt
        RawDataReaderPort.kt
        ChangeSetReaderPort.kt
        ContractRegistryPort.kt
      adapters/
        dynamodb/
          SliceRepositoryAdapter.kt
          InvertedIndexRepositoryAdapter.kt

    view/
      domain/
        ViewDefinition.kt
        ViewRef.kt
        MissingPolicy.kt
        PartialPolicy.kt
        FallbackPolicy.kt
        ViewMeta.kt
      application/
        MaterializeViewUseCase.kt
        ExplainViewUseCase.kt
      ports/
        SliceBatchReaderPort.kt
        ContractRegistryPort.kt
      adapters/
        dynamodb/
          SliceBatchReaderAdapter.kt

    sink/
      domain/
        SinkRuleV1.kt
        SinkPlanV1.kt
        PluginDescriptorV1.kt
        CommitRefV1.kt
        SinkRef.kt
        SinkOrchestrator.kt
      application/
        SinkOrchestratorUseCase.kt
        SinkPlanCompiler.kt
        PluginRouter.kt
      ports/
        SinkRuleRegistryPort.kt
        PluginRegistryPort.kt
        SinkPluginPort.kt
        SliceReaderPort.kt  # slicing 도메인에서 구현
        SliceBatchReaderPort.kt  # view 도메인에서 구현
        ViewReaderPort.kt  # view 도메인에서 구현
      adapters/
        registry/
          SinkRuleRegistryAdapter.kt
          PluginRegistryAdapter.kt
        plugins/
          opensearch/
            OpenSearchIndexerPlugin.kt
            SearchDocMappingDsl.kt
          personalize/
            PersonalizeFeedPublisherPlugin.kt
            RecoFeedCsvCodec.kt

    apps/
      runtimeapi/
        ApiServer.kt
        routes/
          IngestRoutes.kt
          TaskRoutes.kt
        wiring/
          RuntimeApiWiring.kt

      sinkworker/
        SinkWorkerMain.kt
        wiring/
          SinkWorkerWiring.kt

      opscli/
        OpsCliMain.kt

      sinksubscriber/  # 확장 가능 (EventBus 기반)
        SinkSubscriberMain.kt
        adapters/
          kafka/
            KafkaSubscriptionAdapter.kt
        wiring/
          SinkSubscriberWiring.kt

  src/test/kotlin/com/company/ivmlite/
    shared/
    rawdata/
    slicing/
    view/
    sink/
    apps/
```

**11-2. v1+ 멀티모듈 승격 구조 (동일 트리 승격)**

```
ivm-lite/
  settings.gradle.kts
  build.gradle.kts

  modules/
    shared/
    rawdata/
    changeset/
    slicing/
    view/
    sink/
    apps-runtimeapi/
    apps-sinkworker/
    apps-opscli/
    apps-sinksubscriber/  # 확장 가능

  docs/
    rfc/
```

각 모듈 내부는 동일한 헥사고날 트리 유지:

```
modules/sink/src/main/kotlin/com/company/ivmlite/sink/
  domain/
  application/
  ports/
  adapters/
    registry/
    plugins/
      opensearch/
      personalize/
```

**11-3. "subscriber 확장"을 위한 트리거 계층 추가 (열려있음)**

**EventBus subscriber를 붙이는 경우**:

```
src/main/kotlin/com/company/ivmlite/apps/
  sinksubscriber/
    SinkSubscriberMain.kt
    adapters/
      kafka/
        KafkaSubscriptionAdapter.kt
    wiring/
      SinkSubscriberWiring.kt
```

**원칙**:
- subscriber는 이벤트 수신 + taskRef 해석 + SinkOrchestrator 호출만 수행
- sink 실행 로직은 절대 apps에 복제하지 않음
- SinkOrchestrator는 sink 도메인에만 존재

**11-4. 최종 룰 (레포 구조 관점, P0, 필수)**

**11-4-1. 도메인 간 직접 참조 금지**

- `sink.adapters.plugins`가 `slicing.domain`을 직접 import 금지
- 반드시 `SliceReaderPort`/`ViewReaderPort`로만 접근
- 도메인 경계 유지 (RFC-005: 7-1)

**11-4-2. wiring에서만 조립**

- 플러그인 등록, registry 주입은 `shared/wiring` 또는 `apps/*/wiring`에서만
- 도메인 폴더 내부에서 타 도메인 adapter를 new/주입하는 코드 금지

**11-4-3. 정책/계약은 registry로**

- SinkRule/PluginDescriptor/JoinSpec/DependencyMap은 `shared/contract-registry`(또는 그 adapter)로 고정
- Contract Registry에 저장 (RFC-003: 2-16)

**11-5. 플러그인 관리 원칙 (P0, 필수)**

**플러그인 위치**:
- 모든 SinkPlugin은 `sink/adapters/plugins/*` 하위에 위치
- 플러그인별로 독립 디렉터리 (opensearch/, personalize/ 등)

**플러그인 등록**:
- PluginDescriptor는 Contract Registry에 저장
- PluginRegistryPort를 통해 동적 로드 가능
- 런타임에 플러그인 교체/추가 가능

**플러그인 인터페이스**:
- 모든 플러그인은 `SinkPluginPort` 구현
- 입력/출력 계약 고정
- 결정적 실행 보장 (RFC-002: 1-3)

**11-6. 도메인 간 의존성 (RFC-005 준수)**

- sink 도메인은 slicing/view 도메인에 직접 의존하지 않음
- Port를 통해서만 접근 (RFC-005: 7-1)
- 실제 구현 연결은 shared/wiring 또는 apps/*/wiring에서 주입
- 플러그인도 Port를 통해서만 Slice/View 읽기 (직접 DB 접근 금지)

12. 최종 한 문장 정의 (이 RFC의 정체성)

**IVM Sink는 트리거 기반(Task 기반)으로만 동작하며, Slice/View는 Port를 통해 읽고, 실제 외부 upsert/publish는 SinkPlugin이 수행하고, sync/outbox/eventbus는 트리거만 다르고 실행 모델은 동일하다.**

**12-0. 확장성 보장**

설계상 확장성은 열려있음. 나중에 "비동기 subscriber(consumer)가 처리"하는 형태로 자연스럽게 확장 가능함.

**확장 방식**:
1. **Outbox 기반 sinkworker** (이미 포함): runtimeapi가 SinkTask를 outbox에 적재 → sinkworker가 consume → orchestrator 실행
2. **Event Bus 기반 subscriber** (확장 가능): runtimeapi가 SliceBuilt 이벤트 발행 → subscriber가 consume → orchestrator 실행

**SOTA 패턴**: Outbox + EventBus를 같이 둠 (Outbox는 정합/내구/멱등 보장, EventBus는 팬아웃/확장성 담당)

**핵심**: 실행 로직(SinkOrchestrator + SinkPlugin)은 그대로 유지하고, 트리거만 교체 가능하게 설계됨.

**12-1. 구현 가이드**

이 RFC 기준으로:
- 구현 나가도 됨
- ArchUnit/정적 룰 박아도 됨
- SDK/Outbox/Worker 분리해도 구조 안 흔들림

**12-2. 다음 단계**

- SinkRule JSON Schema 실물
- SliceReaderPort/SliceBatchReaderPort 인터페이스 고정 (RFC-005 준수)
- sinkworker retry/backoff 규칙 (RFC-002: 3-4 동시성 제어 전략 준수)
- Outbox 이벤트 재시도 정책 (RFC-002: 3-2 재시도 가능 오류 기준)
- 바로 이어서 확정 가능함

**12-3. Retry/Backoff 규칙 (RFC-002 준수)**

**SinkPlugin 실행 실패 시**:
- Soft Failure (TransientError): Exponential backoff 재시도 (RFC-002: 3-2)
- Hard Failure (InvariantViolation): 즉시 중단, 재시도 금지 (RFC-002: 3-1)
- ConcurrencyConflict: Single-flight 충돌 시 대기 후 재시도 (RFC-002: 3-4)

**Outbox 이벤트 재시도**:
- 최대 재시도 횟수: 설정 가능 (기본 3회)
- Exponential backoff: initialDelay 100ms, maxDelay 1s, multiplier 2.0
- 재시도 실패 시 DLQ 이동 (RFC-004: 12)

13. RFC-001~006과의 일관성 요약

**13-1. 핵심 개념 일치**

- Slice/View 개념 (RFC-001)
- Canonicalization/Hashing 규칙 (RFC-002)
- Contract Registry 기반 관리 (RFC-003)
- Replay/Backfill 연계 (RFC-004)
- Domain-sliced 구조 (RFC-005)
- RefIndexSlice/EnrichmentSlice 연계 (RFC-006)

**13-2. 불변식 준수**

- 결정성 보장 (RFC-002: 1-3)
- 멱등성 보장 (RFC-002: 1-4)
- Fail-closed 정책 (RFC-001: 1-5)
- Port를 통한 도메인 경계 유지 (RFC-005: 7-1)

**13-3. 용어 구분**

- Inverted Index (RFC-001): 런타임 보조 인덱스 (엔티티 간 조인용)
- OpenSearch Index (본 RFC): 검색엔진 인덱스 (검색/추천용)
- 두 개념은 완전히 분리됨 (RFC-001: 17-1)
